#!/usr/bin/env python3
# -*- coding: utf-8 -*-


"""
mask_ls_tasks.py â€”â€” äº§å‡ºä¸æ–¹æ¡ˆ A å®Œå…¨åŒ¹é…çš„ Label Studio æ–‡ä»¶

ç‰¹æ€§ï¼š
- è¾“å‡ºä¸»æ„å›¾ã€æ¬¡æ„å›¾ã€å¤šä¸ªåŒçº§æ„å›¾
- è¾“å‡ºçš„ ls_config.xml ä½¿ç”¨ $text / $weak_primary / $weak_secondary / $weak_same_level / $weak_confidence / $weak_reason
- è¿œç«¯æ¨ç†ï¼ˆOpenAI å…¼å®¹æ¥å£ï¼šOllama / vLLM / å…¶å®ƒï¼‰ï¼Œå¹¶å‘+æ–­ç‚¹ç»­è·‘
- åç±»ä¸­æ–‡æ„å›¾ï¼ˆä¸»æ„å›¾å¿…é€‰ã€æ¬¡æ„å›¾å¯ç©ºï¼ŒåŒçº§æ„å›¾å¯ç©ºï¼‰
"""

from __future__ import annotations
import json
import os
import re
import time
from typing import Any, Iterable, List, Dict, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed

from datasets import load_dataset
from openai import OpenAI
from openai._exceptions import APIStatusError, RateLimitError, APIConnectionError

# --- æ”¾åœ¨æ–‡ä»¶å¾ˆå‰é¢ï¼Œé¿å…è¢«åç»­å¯¼å…¥è¦†ç›– ---
import os
os.environ["NO_PROXY"] = "127.0.0.1,localhost"
os.environ["no_proxy"] = "127.0.0.1,localhost"

# =============== æ¨ç†æœåŠ¡é…ç½®ï¼ˆæŒ‰éœ€ä¿®æ”¹ï¼‰ =================
OPENAI_BASE = "http://192.168.3.47:11434/v1"   # â† ä½ çš„è¿œç«¯æœåŠ¡åœ°å€ï¼ˆOllama / vLLMï¼‰
OPENAI_API_KEY = "EMPTY"                       # Ollama é»˜è®¤å¯ä»»æ„å ä½ï¼›vLLM å¦‚é…ç½®é‰´æƒåˆ™å¡«çœŸå®key
REMOTE_MODEL   = "qwen2.5:14b"                 # ä¸è¿œç«¯æœåŠ¡åŠ è½½çš„æ¨¡å‹åä¸€è‡´
MAX_TOKENS     = 32                           # åˆ†ç±»è¾“å‡º JSONï¼Œ64 è¶³å¤Ÿï¼›æƒ³æ›´å¿«å¯æ”¹ 32
TEMPERATURE    = 0.0                           # åˆ†ç±»ç”¨è´ªå¿ƒè§£ç æ›´ç¨³
CONCURRENCY    = 8                             # å¹¶å‘çº¿ç¨‹æ•°ï¼ŒæŒ‰è¿œç«¯è´Ÿè½½è°ƒæ•´
TIMEOUT_SEC    = 60                            # å•è¯·æ±‚è¶…æ—¶ï¼ˆå¯æŒ‰éœ€è°ƒæ•´ï¼‰
# ========================================================

# =============== æ•°æ®ä¸è¾“å‡º ==============================
DATASET_NAME   = "FreedomIntelligence/huatuo26M-testdatasets"
SPLIT: Optional[str] = None        # None=auto(train>test>validation)
QUESTION_FIELD = "questions"
MAX_SAMPLES: Optional[int] = None  # å…ˆå°æ ·æœ¬éªŒè¯å¯è®¾ 200ï¼›å…¨é‡ç”¨ None
OUT_JSON   = "ls_tasks.json"       # å¯¼å…¥ Label Studio çš„æ•°ç»„ JSONï¼ˆå¸¦ annotationsï¼‰
OUT_JSONL  = "ls_tasks.jsonl"      # æ–­ç‚¹ç»­è·‘å¢é‡æ–‡ä»¶
OUT_CONFIG = "ls_config.xml"       # ä¸æ–¹æ¡ˆ A åŒ¹é…çš„é…ç½®
# ========================================================

# åç±»ä¸­æ–‡æ„å›¾
INTENTS_CN = [
    "å¥åº·å’¨è¯¢",
    "è¯å“æœåŠ¡",
    "æŠ¥å‘Šè§£è¯»",
    "å°±åŒ»è½¬è¯Š",
    "ç´§æ€¥æ±‚åŠ©",
    "ç³»ç»Ÿæ“ä½œ",
    "æƒ…æ„Ÿæ”¯æŒ",
    "å®¶åº­ç®¡ç†",
    "ç¯å¢ƒå¥åº·",
    "é—²èŠå…¶ä»–",
]

# è®©æ¨¡å‹ä¸¥æ ¼è¾“å‡º JSON çš„ç³»ç»Ÿæç¤º
LLM_SYSTEM_PROMPT_CN = f"""ä½ æ˜¯ä¸€ä¸ªä¸­æ–‡åŒ»ç–—æ„å›¾åˆ†ç±»åŠ©ç†ã€‚è¯·åŸºäºç”¨æˆ·ä¸€å¥è¯ï¼Œåœ¨ä¸‹åˆ—åä¸ªä¸­æ–‡æ„å›¾ä¸­åˆ¤æ–­ï¼š
{', '.join(INTENTS_CN)}

è¦æ±‚ï¼š
1) primary å¿…é¡»ä¸”ä»…èƒ½ä»ä¸Šé¢åç±»ä¸­é€‰ä¸€ä¸ªï¼ˆä¸­æ–‡æ ‡ç­¾ï¼Œéœ€å®Œå…¨åŒ¹é…ï¼‰ã€‚
2) secondary ä¸ºä¸Šé¢åç±»ä¸­çš„è‹¥å¹²ä¸ªï¼ˆå¯ä¸ºç©ºæ•°ç»„ï¼‰ï¼ŒåŒæ ·å¿…é¡»æ˜¯ä¸­æ–‡æ ‡ç­¾ã€‚
3) åŒçº§æ„å›¾æ˜¯ä¸ä¸»æ„å›¾åŒç­‰é‡è¦çš„å…¶ä»–æ„å›¾ï¼Œå¯ä»¥æ˜¯å¤šä¸ªï¼Œä¹Ÿå¯ä»¥ä¸ºç©ºæ•°ç»„ã€‚
4) è¾“å‡ºä¸¥æ ¼ä¸º JSONï¼Œå­—æ®µï¼šprimary(å­—ç¬¦ä¸²,ä¸­æ–‡)ã€secondary(å­—ç¬¦ä¸²æ•°ç»„,ä¸­æ–‡)ã€same_level(å­—ç¬¦ä¸²æ•°ç»„,ä¸­æ–‡)ã€confidence(0~1)ã€reason(ä¸­æ–‡,ä¸è¶…è¿‡50å­—)ã€‚
5) åªè¾“å‡º JSONï¼Œä¸è¦å¤šä½™æ–‡æœ¬ã€‚
"""
USER_PROMPT_CN = "ç”¨æˆ·é—®é¢˜ï¼š{q}\nè¯·æŒ‰è¦æ±‚è¾“å‡º JSONã€‚"
JSON_REGEX = re.compile(r"\{.*\}", re.S)


# ----------------- å·¥å…·æ–¹æ³• -----------------
def load_input_dataset() -> Tuple[Dict[str, Any], str]:
    """åŠ è½½æ•°æ®é›†"""
    if os.path.isfile(DATASET_NAME) and DATASET_NAME.lower().endswith(".csv"):
        kwargs = {}
        ds_dict = load_dataset("csv", data_files=DATASET_NAME, **kwargs)
        split = pick_split(ds_dict)
        return ds_dict, split
    else:
        ds_dict = load_dataset(DATASET_NAME)
        split = SPLIT or pick_split(ds_dict)
        return ds_dict, split


def resolve_question_field(dset) -> str:
    """è§£æé—®é¢˜åˆ—å"""
    cols = list(dset.features.keys())
    if QUESTION_FIELD and QUESTION_FIELD in cols:
        return QUESTION_FIELD

    for k in QUESTION_FIELD:
        if k in cols:
            return k

    try:
        from datasets import Value
        for k, v in dset.features.items():
            if isinstance(v, Value) and v.dtype in ("string", "large_string"):
                return k
    except Exception:
        pass
    return cols[0] if cols else QUESTION_FIELD or "text"


def pick_split(ds_dict) -> str:
    keys = list(ds_dict.keys())
    for k in ("train", "test", "validation"):
        if k in keys:
            return k
    return keys[0]


def iter_questions(val: Any) -> Iterable[str]:
    """ä» questions å­—æ®µæŠ½é—®å¥ï¼šå…¼å®¹å¤šç§æ ¼å¼"""
    keys = ("question", "instruction", "input", "query", "prompt")
    if val is None:
        return []
    if isinstance(val, str):
        return [val.strip()] if val.strip() else []
    if isinstance(val, list):
        out = []
        for it in val:
            if isinstance(it, str) and it.strip():
                out.append(it.strip())
            elif isinstance(it, dict):
                for k in keys:
                    v = it.get(k)
                    if isinstance(v, str) and v.strip():
                        out.append(v.strip())
                        break
        return out
    if isinstance(val, dict):
        for k in keys:
            v = val.get(k)
            if isinstance(v, str) and v.strip():
                return [v.strip()]
    return []


def safe_json_extract(text: str) -> Dict[str, Any]:
    """å°½åŠ›ä» text ä¸­æå– JSON"""
    m = JSON_REGEX.search(text)
    if m:
        try:
            return json.loads(m.group(0))
        except Exception:
            pass
    braces = re.findall(r"\{.*\}", text, re.S)
    if braces:
        try:
            return json.loads(braces[-1])
        except Exception:
            pass
    return {"primary": "é—²èŠå…¶ä»–", "secondary": [], "same_level": [], "confidence": 0.5, "reason": "è§£æå¤±è´¥å…œåº•"}


def make_ls_result(primary: str, secondary: List[str], same_level: List[str]) -> List[Dict[str, Any]]:
    """ç”Ÿæˆ Label Studio æ ‡æ³¨ç»“æœ"""
    res = []
    if primary:
        res.append({
            "from_name": "primary",
            "to_name": "text",
            "type": "choices",
            "value": {"choices": [primary]},
        })
    if secondary:
        res.append({
            "from_name": "secondary",
            "to_name": "text",
            "type": "choices",
            "value": {"choices": secondary},
        })
    if same_level:
        res.append({
            "from_name": "same_level",
            "to_name": "text",
            "type": "choices",
            "value": {"choices": same_level},
        })
    return res


def load_done_set(jsonl_path: str) -> set:
    """ä» JSONL é‡ŒåŠ è½½å·²å®Œæˆæ–‡æœ¬é›†åˆï¼ˆæ–­ç‚¹ç»­è·‘ï¼‰"""
    done = set()
    if os.path.isfile(jsonl_path):
        with open(jsonl_path, "r", encoding="utf-8") as f:
            for line in f:
                try:
                    obj = json.loads(line)
                    txt = obj.get("data", {}).get("text")
                    if txt:
                        done.add(txt)
                except Exception:
                    continue
    return done

# å…¶å®ƒéƒ¨åˆ†çš„ä»£ç ...

def build_ls_config_xml() -> str:
    """Label Studio é…ç½®ï¼ˆä¸»æ„å›¾å•é€‰å¿…é€‰ + æ¬¡æ„å›¾å¤šé€‰å¯ç©º + åŒçº§æ„å›¾å¤šé€‰å¯ç©ºï¼‰â€”â€”ä¸è‹±æ–‡é”®å¯¹é½"""
    choices_xml = "\n    ".join(f'<Choice value="{v}">{v}</Choice>' for v in INTENTS_CN)
    return f"""<View>
  <!-- å¾…æ ‡æ³¨æ–‡æœ¬ï¼ˆè‹±æ–‡é”® $textï¼‰ -->
  <Text name="text" value="$text" />

  <!-- ä¸»æ„å›¾ï¼šå¿…é€‰Â·å•é€‰ -->
  <Header value="è¯·é€‰æ‹©ã€ä¸»æ„å›¾ã€‘ï¼ˆå¿…é€‰Â·å•é€‰ï¼‰" />
  <Choices name="primary" toName="text" choice="single" required="true" showInLine="true">
    {choices_xml}
  </Choices>

  <!-- æ¬¡æ„å›¾ï¼šå¯å¤šé€‰Â·å¯ç•™ç©º -->
  <Header value="å¯é€‰ï¼šè¯·é€‰æ‹©ã€æ¬¡æ„å›¾ã€‘ï¼ˆå¯å¤šé€‰Â·å¯ç•™ç©ºï¼‰" />
  <Choices name="secondary" toName="text" choice="multiple" showInLine="true">
    {choices_xml}
  </Choices>

  <!-- åŒçº§æ„å›¾ï¼šå¯å¤šé€‰Â·å¯ç•™ç©º -->
  <Header value="å¯é€‰ï¼šè¯·é€‰æ‹©ã€åŒçº§æ„å›¾ã€‘ï¼ˆå¯å¤šé€‰Â·å¯ç•™ç©ºï¼‰" />
  <Choices name="same_level" toName="text" choice="multiple" showInLine="true">
    {choices_xml}
  </Choices>

  <!-- é¢„æ ‡æ³¨ä¿¡æ¯ï¼ˆä»…å±•ç¤ºï¼Œä¸å‚ä¸ toName å¼•ç”¨ï¼‰ -->
  <Header value="â€”â€” é¢„æ ‡æ³¨ï¼ˆä»…å‚è€ƒï¼‰ â€”â€”" />
  <Text name="weak1" value="å¼±ä¸»æ„å›¾ï¼š$weak_primary" />
  <Text name="weak2" value="å¼±æ¬¡æ„å›¾ï¼š$weak_secondary" />
  <Text name="weak3" value="å¼±åŒçº§æ„å›¾ï¼š$weak_same_level" />
  <Text name="weak4" value="ç½®ä¿¡åº¦ï¼š$weak_confidence" />
  <Text name="weak5" value="ç†ç”±ï¼š$weak_reason" />
</View>"""

# ä¸»æµç¨‹...

# ----------------- è¿œç¨‹æ¨ç†å®¢æˆ·ç«¯ -----------------
client = OpenAI(
    base_url=OPENAI_BASE,
    api_key=OPENAI_API_KEY,
    timeout=TIMEOUT_SEC,
)

def classify_remote(q: str, retry: int = 3, backoff: float = 1.5) -> Tuple[str, List[str], List[str], float, str]:
    """è°ƒç”¨è¿œç¨‹æ¨ç†æ¥å£è¿›è¡Œåˆ†ç±»"""
    system = {"role": "system", "content": LLM_SYSTEM_PROMPT_CN}
    user   = {"role": "user", "content": USER_PROMPT_CN.format(q=q)}
    last_err = None
    for k in range(retry):
        try:
            rsp = client.chat.completions.create(
                model=REMOTE_MODEL,
                messages=[system, user],
                temperature=TEMPERATURE,
                max_tokens=MAX_TOKENS,
                stream=False,
            )
            text = rsp.choices[0].message.content or ""
            data = safe_json_extract(text)

            primary = str(data.get("primary", "")).strip()
            secondary = data.get("secondary") or []
            same_level = data.get("same_level") or []

            if primary not in INTENTS_CN:
                primary = "é—²èŠå…¶ä»–"
            secondary = [s for s in secondary if s in INTENTS_CN]
            same_level = [s for s in same_level if s in INTENTS_CN]

            conf = float(data.get("confidence", 0.7))
            reason = str(data.get("reason", "")).strip()[:100]
            return (primary, secondary, same_level, max(0.0, min(1.0, conf)), reason)
        except (APIStatusError, RateLimitError, APIConnectionError, TimeoutError) as e:
            last_err = e
            time.sleep((k + 1) * backoff)
        except Exception as e:
            last_err = e
            time.sleep((k + 1) * backoff)
    return ("é—²èŠå…¶ä»–", [], [], 0.5, f"è¿œç«¯é”™è¯¯ï¼š{last_err}"[:100])


# ----------------- ä¸»æµç¨‹ -----------------
def main():
    print(f"ğŸ“¦ Loading dataset: {DATASET_NAME}")
    ds_dict, split = load_input_dataset()
    dset = ds_dict[split]
    print(f"âœ… Using split: {split}, rows={len(dset)}; remote_model={REMOTE_MODEL}")

    q_field = resolve_question_field(dset)
    seen = set()
    questions = []
    for rec in dset:
        val = rec.get(q_field, None)
        for q in iter_questions(val):
            if q and q not in seen:
                seen.add(q)
                questions.append(q)
        if MAX_SAMPLES and len(questions) >= MAX_SAMPLES:
            break

    print(f"ğŸ“ Extracted questions: {len(questions)}")

    done = load_done_set(OUT_JSONL)
    if done:
        print(f"ğŸ” æ–­ç‚¹ç»­è·‘ï¼šæ£€æµ‹åˆ°å·²å®Œæˆ {len(done)} æ¡ï¼Œå°†è·³è¿‡è¿™äº›æ ·æœ¬ã€‚")

    jsonl_f = open(OUT_JSONL, "a", encoding="utf-8")
    processed = 0
    start_time = time.time()

    to_process = [q for q in questions if q not in done]
    print(f"ğŸš€ Start remote inference: {len(to_process)} to process, concurrency={CONCURRENCY}")
    buf_tasks: List[Dict[str, Any]] = []

    with ThreadPoolExecutor(max_workers=CONCURRENCY) as ex:
        fut2q = {ex.submit(classify_remote, q): q for q in to_process}
        for fut in as_completed(fut2q):
            q = fut2q[fut]
            primary, secondary, same_level, conf, reason = fut.result()

            data = {
                "text": q,
                "weak_primary": primary,
                "weak_secondary": "ã€".join(secondary) if secondary else "",
                "weak_same_level": "ã€".join(same_level) if same_level else "",
                "weak_confidence": float(conf),
                "weak_reason": reason
            }
            task = {"data": data}
            task["annotations"] = [{
                "result": make_ls_result(primary, secondary, same_level),
                "was_cancelled": False,
                "ground_truth": False,
            }]

            jsonl_f.write(json.dumps(task, ensure_ascii=False) + "\n")
            jsonl_f.flush()

            buf_tasks.append(task)
            processed += 1

            if processed % 50 == 0:
                elapsed = time.time() - start_time
                speed = processed / max(1e-9, elapsed)
                print(f"ğŸ§© Prelabelled {processed}/{len(to_process)} | {speed:.2f} samples/s | elapsed {elapsed/60:.1f} min")

    jsonl_f.close()

    tasks = []
    with open(OUT_JSONL, "r", encoding="utf-8") as f:
        for line in f:
            try:
                tasks.append(json.loads(line))
            except Exception:
                continue

    with open(OUT_JSON, "w", encoding="utf-8") as f:
        json.dump(tasks, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ Wrote tasks (array JSON): {OUT_JSON}")

    with open(OUT_CONFIG, "w", encoding="utf-8") as f:
        f.write(build_ls_config_xml())
    print(f"âš™ï¸  Wrote LS config: {OUT_CONFIG}")

    total_elapsed = time.time() - start_time
    print(f"âœ… å®Œæˆï¼šå¤„ç† {processed}/{len(to_process)} æ¡ | æ€»è€—æ—¶ {total_elapsed/60:.1f} åˆ†é’Ÿ | å¹¶å‘ {CONCURRENCY}")


if __name__ == "__main__":
    main()

# -*- coding: utf-8 -*-
"""
rag_llm_cli.py
äº¤äº’å¼ï¼šæ„å›¾è¯†åˆ« â†’ å¤šçŸ¥è¯†åº“æ£€ç´¢(RAG) â†’ LLM ç”Ÿæˆï¼ˆå¸¦å¼•ç”¨ï¼‰
- å‘½ä¸­ä¸ºç©º/ä½ç½®ä¿¡ï¼šä¸ç”Ÿæˆï¼Œç›´æ¥æç¤ºæ²¡æœ‰æŠŠæ¡
- è¾“å…¥ exit / quit / q é€€å‡º
"""
import argparse
from typing import List, Dict, Any, Optional

# ========= 1) ç»„ä»¶åŠ è½½ =========
def load_components(device: Optional[str] = None, use_bm25: bool = True, use_reranker: bool = True):
    from process.intent_adapter import predict_intent
    from modules.retriever.rag_retriever import INTENT_TO_KB, KnowledgeSearcher, DenseEncoder, _build_reranker
    # ç®€å•çš„ Searcher ç¼“å­˜
    _cache = {}
    def get_searcher(kb_name: str):
        if kb_name in _cache:
            return _cache[kb_name]
        embedder = DenseEncoder(device=device)
        reranker = _build_reranker(device=device) if use_reranker else None
        searcher = KnowledgeSearcher(embedder, reranker, use_bm25=use_bm25, kb_name=kb_name)
        _cache[kb_name] = searcher
        return searcher
    return predict_intent, INTENT_TO_KB, get_searcher

# ========= 2) LLM å®¢æˆ·ç«¯ï¼ˆOpenAI å…¼å®¹ / Ollama / è‡ªå®šä¹‰HTTPï¼Œè‡ªåŠ¨é€‚é…ï¼‰=========
import os, requests, time

def get_llm_mode():
    """
    ä»ç¯å¢ƒå˜é‡ç¡®å®šè°ƒç”¨æ¨¡å¼ï¼š
      LLM_MODE = openai | ollama | http
    é»˜è®¤ä¼˜å…ˆ openaiï¼ˆè‹¥è®¾ç½®äº† OPENAI_API_KEYï¼‰ï¼Œå¦åˆ™ ollamaï¼ˆæœ¬åœ°11434ï¼‰ï¼Œå¦åˆ™ httpã€‚
    """
    mode = os.getenv("LLM_MODE")
    if mode:
        return mode.lower()
    if os.getenv("OPENAI_API_KEY"):
        return "openai"
    if os.getenv("LLM_API_BASE_URL", "http://localhost:11434"):
        return "ollama"
    return "http"


def get_llm_client():
    """
    è¿”å›ä¸€ä¸ªç»Ÿä¸€çš„ callable: generate(prompt:str) -> str
    æ”¯æŒä¸‰ç§æ¨¡å¼ï¼š
      - openai: éœ€ OPENAI_API_KEYï¼ˆå¯é€‰ OPENAI_BASE_URLï¼‰ï¼Œæ¨¡å‹åç”¨ RAG_LLM_MODEL
      - ollama: é»˜è®¤ http://localhost:11434, æ¥å£ /api/generate, æ¨¡å‹åç”¨ RAG_LLM_MODELï¼ˆå¦‚ qwen2.5:14bï¼‰
      - http:   é€šç”¨POSTåˆ° LLM_API_BASE_URLï¼ˆéœ€ä½ è‡ªå®šä¹‰åç«¯æ¥æ”¶ {prompt, model}ï¼‰
    """
    mode = get_llm_mode()
    model_name = os.getenv("RAG_LLM_MODEL", "qwen2.5:14b")

    if mode == "openai":
        from openai import OpenAI
        base_url = os.getenv("OPENAI_BASE_URL")  # å¯ä¸ºç©º
        api_key = os.getenv("OPENAI_API_KEY")
        client = OpenAI(api_key=api_key, base_url=base_url) if base_url else OpenAI(api_key=api_key)

        def _gen(prompt: str, temperature=0.2, max_tokens=700):
            resp = client.chat.completions.create(
                model=model_name,
                messages=[{"role":"user","content": prompt}],
                temperature=temperature,
                max_tokens=max_tokens,
            )
            return (resp.choices[0].message.content or "").strip()

        return _gen

    elif mode == "ollama":
        base_url = os.getenv("LLM_API_BASE_URL", "http://localhost:11434").rstrip("/")
        timeout = float(os.getenv("LLM_HTTP_TIMEOUT", "120"))

        def _gen(prompt: str, temperature=0.2, max_tokens=700):
            # Ollama çš„ /api/generate é»˜è®¤æ˜¯æµå¼ï¼›åŠ  stream:false å¾—åˆ°å®Œæ•´æ–‡æœ¬
            payload = {
                "model": model_name,
                "prompt": prompt,
                "options": {
                    "temperature": temperature,
                    "num_predict": max_tokens
                },
                "stream": False
            }
            r = requests.post(f"{base_url}/api/generate", json=payload, timeout=timeout)
            r.raise_for_status()
            data = r.json()
            # Ollama å¸¸è§å­—æ®µï¼šresponse
            text = data.get("response") or data.get("text") or data.get("output") or ""
            return text.strip()

        return _gen

    else:  # é€šç”¨ HTTPï¼šPOST åˆ° LLM_API_BASE_URL
        base_url = os.getenv("LLM_API_BASE_URL")
        if not base_url:
            raise RuntimeError("LLM_MODE=http æ—¶å¿…é¡»è®¾ç½® LLM_API_BASE_URL")
        base_url = base_url.rstrip("/")
        timeout = float(os.getenv("LLM_HTTP_TIMEOUT", "120"))

        def _gen(prompt: str, temperature=0.2, max_tokens=700):
            payload = {"prompt": prompt, "model": model_name,
                       "temperature": temperature, "max_tokens": max_tokens}
            r = requests.post(base_url, json=payload, timeout=timeout)
            r.raise_for_status()
            data = r.json()
            # å¸¸è§åç«¯çš„å‡ ç§å­—æ®µå
            text = (data.get("text") or data.get("output") or data.get("data") or "").strip()
            return text

        return _gen

# ========= 3) Prompt æ¨¡æ¿ï¼ˆæ›´å¼ºé˜²å¹»è§‰+ç»“æ„åŒ–+å¼•ç”¨ï¼‰=========
def build_prompt(query: str, passages: List[Dict[str, Any]]) -> str:
    """
    ç»“æ„åŒ–æç¤ºï¼š
      - åªå…è®¸ä¾æ®è¯æ®å›ç­”ï¼›ä¸è¶³åˆ™æ˜ç¡®è¯´æ— æ³•ç¡®å®š
      - å…ˆç»™è¦ç‚¹ï¼Œå†ç»™å»ºè®®ï¼Œæœ€ååˆ—å¼•ç”¨ [S1][S2]â€¦
    """
    def norm(x: str) -> str:
        return (x or "").replace("\n", " ").strip()
    blocks = []
    for i, p in enumerate(passages, 1):
        blocks.append(f"[S{i}] {norm(p.get('text'))}  (source: {p.get('source','')})")
    context_block = "\n".join(blocks)

    return f"""ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„ä¸­æ–‡åŒ»å­¦åŠ©ç†ã€‚è¯·ä»…ä¾æ®â€œèµ„æ–™â€ä½œç­”ï¼Œä¸å¾—ç¼–é€ èµ„æ–™ä¸­æ²¡æœ‰çš„ç»“è®ºã€‚
è‹¥èµ„æ–™ä¸è¶³ä»¥å›ç­”ï¼Œè¯·æ˜ç¡®è¯´æ˜â€œæ ¹æ®ç°æœ‰èµ„æ–™æ— æ³•ç»™å‡ºç¡®å®šç­”æ¡ˆâ€ã€‚

# é—®é¢˜
{query}

# èµ„æ–™ï¼ˆå¯èƒ½ä¸å®Œæ•´ï¼‰
{context_block}

# ä½œç­”è¦æ±‚ï¼ˆä¸¥æ ¼éµå®ˆï¼‰
1. å…ˆç”¨ 3â€“5 å¥ç»™å‡ºâ€œæ ¸å¿ƒç»“è®ºâ€ï¼Œè¯­è¨€ç®€æ´ã€‚
2. è‹¥æ¶‰åŠç”¨è¯æˆ–æŠ¥å‘Šåˆ¤è¯»ï¼Œå¿…é¡»æç¤ºâ€œéœ€ç»“åˆä¸ªä½“æƒ…å†µå¹¶åœ¨åŒ»ç”ŸæŒ‡å¯¼ä¸‹è¿›è¡Œâ€ã€‚
3. ä¸è¦å¼•ç”¨å¤–éƒ¨å¸¸è¯†ï¼Œ**åª**èƒ½ä½¿ç”¨èµ„æ–™ä¸­çš„ä¿¡æ¯ã€‚
4. è‹¥æ— æ³•ç¡®å®šï¼Œè¯·ç›´è¯´æ— æ³•ç¡®å®šï¼Œä¸è¦çŒœæµ‹ã€‚
5. ç»“å°¾ç»™å‡ºâ€œå‚è€ƒæ¥æºâ€ï¼Œæ ¼å¼å¦‚ï¼šå‚è€ƒï¼š[S1][S3]ã€‚
"""
# ========= 4) ç”Ÿæˆç­”æ¡ˆï¼ˆå¢åŠ é‡è¯•+ç©ºä¸²å…œåº•ä¸ºæŠ½å–å¼ï¼‰=========
def _extractive_fallback(query: str, hits: List[Dict[str, Any]], max_sents: int = 5) -> str:
    """
    ç®€å•æŠ½å–å¼å…œåº•ï¼šä»å‘½ä¸­æ–‡æ®µé‡ŒæŠ½å–å‰å‡ å¥ï¼Œæ‹¼æˆå›ç­”ï¼Œå¹¶è¿½åŠ å¼•ç”¨ã€‚
    """
    import re
    def sent_split(t: str):
        t = (t or "").strip()
        parts = re.split(r"[ã€‚ï¼ï¼Ÿ!?ï¼›;]\s*", t)
        return [p for p in parts if p]
    sents = []
    for h in hits:
        sents.extend(sent_split(h.get("text","")))
        if len(sents) >= max_sents:
            break
    sents = sents[:max_sents]
    refs = "å‚è€ƒï¼š" + "".join([f"[S{i+1}]" for i in range(len(hits))]) if hits else ""
    if not sents:
        return "æ ¹æ®ç°æœ‰èµ„æ–™æ— æ³•ç»™å‡ºç¡®å®šç­”æ¡ˆã€‚"
    return "ï¼›".join(sents) + "ã€‚ " + refs


def generate_answer(query: str, hits: List[Dict[str, Any]]) -> str:
    """
    è°ƒç”¨æœ¬åœ°/è¿œç«¯ LLM ç”Ÿæˆç­”æ¡ˆï¼›è‹¥è¿”å›ç©ºæˆ–æŠ¥é”™ï¼Œå›é€€åˆ°æŠ½å–å¼å›ç­”ã€‚
    """
    client = get_llm_client()
    prompt = build_prompt(query, hits)

    # ç®€å•é‡è¯•æœºåˆ¶
    for attempt in range(2):
        try:
            text = client(prompt)  # ç»Ÿä¸€ callable
            if text and text.strip():
                return text.strip()
        except Exception as e:
            if attempt == 0:
                time.sleep(0.8)
                continue
            # è®°å½•é”™è¯¯åˆ°æ§åˆ¶å°å³å¯ï¼ˆä¹Ÿå¯ä»¥å†™æ—¥å¿—ï¼‰
            print(f"[LLM Error] {e}")

    # å…œåº•ï¼šæŠ½å–å¼å›ç­”
    return _extractive_fallback(query, hits)

# ========= 5) ä¸»æµç¨‹ï¼šæ„å›¾ â†’ æ£€ç´¢ â†’ åˆ¤ç©º â†’ ç”Ÿæˆ =========
def answer_once(query: str,
                predict_intent,
                intent_to_kb: Dict[str, Optional[str]],
                get_searcher,
                conf_th: float = 0.4,
                top_k_final: int = 4) -> None:
    label, conf, topk = predict_intent(query)
    routed = label if conf >= conf_th else "é—²èŠå…¶ä»–"
    kb_name = intent_to_kb.get(routed, None)

    print(f"â¡ï¸ ä¸»æ„å›¾ï¼š{label}ï¼ˆ{conf:.3f}ï¼‰")
    if topk:
        tops = " | ".join([f"{l}:{p:.3f}" for l,p in topk[:5]])
        print(f"   TopKï¼š{tops}")
    print("   è·¯ç”±åˆ° KBï¼š", kb_name or "ä¸æ£€ç´¢")

    if not kb_name:
        print("ğŸ’¬ å½“å‰æ„å›¾ä¸èµ° KBï¼ˆé€šå¸¸äº¤ç»™ LLM é—²èŠæˆ–å…¶ä»–ä¸šåŠ¡ï¼‰ï¼Œæ­¤å¤„ä¸æ¼”ç¤ºã€‚")
        return

    searcher = get_searcher(kb_name)
    hits = searcher.search(query, top_k_final=top_k_final)

    if not hits:
        print("ğŸ¤” æˆ‘æ²¡æœ‰æŠŠæ¡èƒ½ä»çŸ¥è¯†åº“é‡Œç»™å‡ºæ­£ç¡®ç­”æ¡ˆã€‚å»ºè®®æ¢ä¸€ç§é—®æ³•æˆ–è¡¥å……æ›´å¤šä¿¡æ¯ã€‚")
        return

    print("ğŸ” å·²æ£€ç´¢åˆ°é«˜ç½®ä¿¡ç‰‡æ®µï¼Œç”Ÿæˆç­”æ¡ˆä¸­â€¦")
    ans = generate_answer(query, hits)
    print("\n====== ç­”æ¡ˆ ======")
    print(ans)
    print("\n====== è¯æ® ======")
    for i, h in enumerate(hits, 1):
        src = h.get("source", "")
        brief = h.get("text","").replace("\n"," ")
        if len(brief) > 160: brief = brief[:160] + "â€¦"
        print(f"[S{i}] {brief}  ({src})")


# ========= 6) CLI =========
def main():
    parser = argparse.ArgumentParser("RAG+LLM å‘½ä»¤è¡Œï¼ˆå¸¦æ„å›¾è¯†åˆ«ä¸å¤šåº“è·¯ç”±ï¼‰")
    parser.add_argument("--device", type=str, default=None, help="cuda æˆ– cpu")
    parser.add_argument("--no_bm25", action="store_true", help="å…³é—­ BM25")
    parser.add_argument("--no_reranker", action="store_true", help="å…³é—­è·¨ç¼–ç å™¨é‡æ’")
    parser.add_argument("--conf_th", type=float, default=0.40, help="æ„å›¾ç½®ä¿¡åº¦é˜ˆå€¼")
    parser.add_argument("--once", type=str, default=None, help="åªç­”ä¸€æ¬¡æŒ‡å®šé—®é¢˜")
    args = parser.parse_args()

    predict_intent, intent_to_kb, get_searcher = load_components(
        device=args.device,
        use_bm25=(not args.no_bm25),
        use_reranker=(not args.no_reranker),
    )

    if args.once:
        answer_once(args.once, predict_intent, intent_to_kb, get_searcher, conf_th=args.conf_th)
        return

    print("\n================== RAG + LLM Â· äº¤äº’æ¨¡å¼ ==================")
    print("è¾“å…¥é—®é¢˜å›è½¦ï¼›exit/quit/q é€€å‡ºã€‚")
    print("=========================================================\n")
    try:
        while True:
            q = input(">>> ").strip()
            if not q:
                continue
            if q.lower() in {"exit","quit","q"}:
                print("ğŸ‘‹ å†è§ï¼"); break
            answer_once(q, predict_intent, intent_to_kb, get_searcher, conf_th=args.conf_th)
            print()
    except (KeyboardInterrupt, EOFError):
        print("\nğŸ‘‹ å†è§ï¼")

if __name__ == "__main__":
    # éœ€è¦ OPENAI_API_KEYï¼›å¦‚ç”¨ç§æœ‰æ¨ç†ï¼Œè®¾ç½® OPENAI_BASE_URL + å¯¹åº” key
    main()

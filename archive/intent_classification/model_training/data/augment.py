import os
import json
import uuid
import copy
import random
import asyncio
import aiohttp
import time
from collections import defaultdict, Counter
from typing import List, Dict, Tuple

# =============== é…ç½®åŒºåŸŸï¼ˆæŒ‰éœ€ä¿®æ”¹ï¼‰ ===============
FILES = ["data/intent/train.json", "data/intent/dev.json", "data/intent/test.json"]         # æºæ•°æ®æ–‡ä»¶ï¼ˆä¹Ÿå¯åªæ”¾ä¸€ä¸ª dataset.jsonï¼‰
PRIMARY_FROM_NAMES = {"primary", "ä¸»æ„å›¾"}               # ä½œä¸ºâ€œä¸»æ„å›¾â€çš„ from_name
TARGET_PER_CLASS = 1000                                  # â­ æ¯ä¸ªç±»åˆ«æœ€ç»ˆæƒ³è¦çš„å›ºå®šæ¡æ•°
MAX_GEN_PER_SOURCE = 3                                   # æ¯æ¡åŸæ ·æœ¬æœ€å¤šç”Ÿæˆå¤šå°‘æ¡å¢å¼ºæ ·æœ¬
RANDOM_SEED = 42
SPLIT_RATIOS = (0.7, 0.2, 0.1)
OUTPUT_PREFIX = "qwen_fixed_async"                       # è¾“å‡ºå‰ç¼€ï¼šqwen_fixed_async_train.json ç­‰

# â€”â€” åƒé—® / OpenAI å…¼å®¹æ¥å£ â€”â€”
API_BASE = "http://127.0.0.1:11434/v1"                    # ä½ çš„æœ¬åœ°æ¨ç†æœåŠ¡åœ°å€ï¼ˆä¿æŒ /v1ï¼‰
API_KEY = os.environ.get("OPENAI_API_KEY", "EMPTY")      # è‹¥ä¸éœ€è¦é‰´æƒå¯éšæ„
MODEL = "qwen2.5:14b"                           # ä½ çš„æ¨¡å‹å
USE_CHAT_COMPLETIONS = True                              # True: /chat/completionsï¼›False: /completions

# ç”Ÿæˆå‚æ•°
TEMPERATURE = 0.9
TOP_P = 0.95
MAX_TOKENS = 512

# å¹¶å‘ä¸é‡è¯•
CONCURRENCY = 8             # å¹¶å‘è¯·æ±‚æ•°
TIMEOUT_SEC = 120           # å•è¯·æ±‚è¶…æ—¶
MAX_RETRIES = 3             # æœ€å¤§é‡è¯•æ¬¡æ•°
BACKOFF_BASE = 1.5          # é€€é¿ç³»æ•°
# ====================================================

random.seed(RANDOM_SEED)

# ----------- æ•°æ®è¯»å†™ä¸æ ‡ç­¾/æ–‡æœ¬æŠ½å– -----------
def load_all(files: List[str]) -> List[dict]:
    data = []
    for fp in files:
        try:
            with open(fp, "r", encoding="utf-8") as f:
                part = json.load(f)
                if isinstance(part, list):
                    data.extend(part)
                else:
                    print(f"âš ï¸ {fp} ä¸æ˜¯ JSON æ•°ç»„ï¼Œå·²è·³è¿‡")
        except FileNotFoundError:
            print(f"âš ï¸ æ–‡ä»¶æœªæ‰¾åˆ°ï¼š{fp}")
    return data

def get_primary_labels(item: dict, primary_from_names: set) -> List[str]:
    labels = []
    for ann in item.get("annotations", []):
        for res in ann.get("result", []):
            if res.get("type") == "choices" and res.get("from_name") in primary_from_names:
                labels.extend(res.get("value", {}).get("choices", []))
    return labels

def get_text_field(item: dict) -> Tuple[str, str]:
    if isinstance(item.get(""), dict) and "text" in item["data"]:
        return "data.text", item["data"]["text"]
    if "text" in item:
        return "text", item["text"]
    if isinstance(item.get(""), dict):
        for k, v in item["data"].items():
            if isinstance(v, str):
                return f"data.{k}", v
    return "text", ""

def set_text_field(item: dict, path: str, new_text: str):
    if path.startswith("data.") and "data" in item and isinstance(item["data"], dict):
        key = path.split(".", 1)[1]
        item["data"][key] = new_text
    else:
        item["text"] = new_text

def clone_with_new_text(item: dict, new_text: str, new_id: int) -> dict:
    new_item = copy.deepcopy(item)
    new_item["id"] = new_id
    new_item["uid"] = str(uuid.uuid4())
    path, _ = get_text_field(new_item)
    set_text_field(new_item, path, new_text)
    for ann in new_item.get("annotations", []):
        ann["unique_id"] = str(uuid.uuid4())
    return new_item


# ---------------- Prompt æ¨¡æ¿ ----------------
SYSTEM_PROMPT = (
    "ä½ æ˜¯æ•°æ®å¢å¼ºåŠ©æ‰‹ã€‚è¯·æ”¹å†™ç”¨æˆ·ç»™å‡ºçš„æ–‡æœ¬ï¼Œä¿æŒä¸»æ„å›¾ä¸€è‡´ï¼Œä¸æ”¹å˜äº‹å®ï¼Œä¸æœæ’°ä¿¡æ¯ã€‚\n"
    "è¦æ±‚ï¼š\n"
    "1) ä¸æ”¹å˜ç±»åˆ«æ„å›¾ä¸å…³é”®ä¿¡æ¯ï¼›\n"
    "2) é€‚åº¦åŒä¹‰æ›¿æ¢ã€æ”¹å†™å¥å¼ï¼›\n"
    "3) ä¸æ·»åŠ ä¸åŸæ–‡å†²çªçš„æ–°äº‹å®ï¼›\n"
    "4) è¯­è¨€è‡ªç„¶æµç•…ï¼Œä¿æŒåŸè¯­è¨€é£æ ¼ï¼ˆä¸­æ–‡/ä¸­è‹±æ··æ’å‡å¯ï¼‰ã€‚\n"
    "åªè¾“å‡ºå¢å¼ºåçš„æ–‡æœ¬ï¼Œä¸è¦åŒ…å«åºå·æˆ–è§£é‡Šã€‚"
)

USER_PROMPT_TPL = (
    "è¯·åŸºäºä¸‹è¿°æ–‡æœ¬ç”Ÿæˆ {k} æ¡æ”¹å†™ç‰ˆæœ¬ï¼Œä¿æŒä¸»æ„å›¾ã€Š{label}ã€‹ä¸å˜ï¼š\n"
    "ã€åŸæ–‡ã€‘{text}\n"
    "è¾“å‡ºæ ¼å¼ï¼šæ¯è¡Œä¸€ä¸ªæ”¹å†™æ ·æœ¬ï¼Œä¸åŠ åºå·ã€‚"
)

# ---------------- å¼‚æ­¥ HTTP è°ƒç”¨ ----------------
def _headers():
    return {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {API_KEY}"
    }

async def _post_json(session: aiohttp.ClientSession, url: str, payload: dict, timeout: int) -> dict:
    for attempt in range(MAX_RETRIES):
        try:
            async with session.post(url, json=payload, headers=_headers(), timeout=timeout) as resp:
                if resp.status >= 400:
                    text = await resp.text()
                    raise RuntimeError(f"HTTP {resp.status}: {text[:200]}")
                return await resp.json()
        except Exception as e:
            if attempt == MAX_RETRIES - 1:
                raise
            delay = BACKOFF_BASE * (attempt + 1)
            print(f"âš ï¸ è¯·æ±‚å¤±è´¥ï¼ˆ{e}ï¼‰ï¼Œ{delay:.1f}s åé‡è¯•...")
            await asyncio.sleep(delay)

async def call_qwen_generate_async(
    session: aiohttp.ClientSession,
    text: str,
    label: str,
    k: int
) -> List[str]:
    """ä¸€æ¬¡è°ƒç”¨å°½é‡è¿”å› k æ¡ï¼Œè‹¥ä¸è¶³ç”±ä¸Šå±‚å†³å®šæ˜¯å¦å†è¡¥ã€‚"""
    url = f"{API_BASE}/chat/completions" if USE_CHAT_COMPLETIONS else f"{API_BASE}/completions"
    prompt_user = USER_PROMPT_TPL.format(k=k, label=label, text=text)

    if USE_CHAT_COMPLETIONS:
        payload = {
            "model": MODEL,
            "messages": [
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": prompt_user},
            ],
            "temperature": TEMPERATURE,
            "top_p": TOP_P,
            "max_tokens": MAX_TOKENS,
            "n": 1
        }
    else:
        payload = {
            "model": MODEL,
            "prompt": f"{SYSTEM_PROMPT}\n\n{prompt_user}",
            "temperature": TEMPERATURE,
            "top_p": TOP_P,
            "max_tokens": MAX_TOKENS,
            "n": 1
        }

    data = await _post_json(session, url, payload, TIMEOUT_SEC)
    content = (
        data["choices"][0]["message"]["content"].strip()
        if USE_CHAT_COMPLETIONS else
        data["choices"][0]["text"].strip()
    )
    lines = [ln.strip() for ln in content.splitlines() if ln.strip()]
    uniq, seen = [], set()
    for ln in lines:
        if ln and ln != text and ln not in seen:
            uniq.append(ln)
            seen.add(ln)
    return uniq[:k]

async def call_qwen_generate_single_async(
    session: aiohttp.ClientSession,
    text: str,
    label: str
) -> str:
    """è¡¥é½æ—¶çš„å•æ¡è°ƒç”¨ã€‚"""
    res = await call_qwen_generate_async(session, text, label, 1)
    return res[0] if res else ""

# -------------- å¹¶å‘ä»»åŠ¡ç¼–æ’ --------------
class GenTask:
    """è¡¨ç¤ºå¯¹æŸä¸ªæºæ ·æœ¬éœ€è¦ç”Ÿæˆçš„æ¡ç›®æ•°"""
    __slots__ = ("item", "label", "need")
    def __init__(self, item, label, need):
        self.item = item
        self.label = label
        self.need = need

async def process_task(
    sem: asyncio.Semaphore,
    session: aiohttp.ClientSession,
    task: GenTask,
    next_id_ref: dict
) -> List[dict]:
    """å¤„ç†å•ä¸ªç”Ÿæˆä»»åŠ¡ï¼Œè¿”å›ç”Ÿæˆçš„æ–°æ ·æœ¬åˆ—è¡¨"""
    out_items = []
    async with sem:
        path, src_text = get_text_field(task.item)
        if not src_text or task.need <= 0:
            return out_items

        # å…ˆå°è¯•ä¸€æ¬¡æ€§ç”Ÿæˆ need æ¡
        gens = await call_qwen_generate_async(session, src_text, task.label, task.need)

        # è‹¥ä¸è¶³ï¼Œåˆ™å•æ¡è¡¥é½
        while len(gens) < task.need:
            extra = await call_qwen_generate_single_async(session, src_text, task.label)
            if not extra:
                break
            gens.append(extra)

        # è½¬ä¸ºæ ·æœ¬å¯¹è±¡
        for g in gens[:task.need]:
            nid = next_id_ref["id"]
            next_id_ref["id"] += 1
            out_items.append(clone_with_new_text(task.item, g, nid))
    return out_items

async def run_parallel_generation(tasks: List[GenTask], start_id: int) -> List[dict]:
    """å¹¶å‘æ‰§è¡Œæ‰€æœ‰ç”Ÿæˆä»»åŠ¡"""
    connector = aiohttp.TCPConnector(limit=None)
    timeout = aiohttp.ClientTimeout(total=None)
    sem = asyncio.Semaphore(CONCURRENCY)
    next_id_ref = {"id": start_id}

    async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
        coros = [process_task(sem, session, t, next_id_ref) for t in tasks]
        results = []
        done = 0
        for f in asyncio.as_completed(coros):
            try:
                res = await f
                results.extend(res)
            except Exception as e:
                print(f"âŒ å­ä»»åŠ¡å¤±è´¥ï¼š{e}")
            finally:
                done += 1
                if done % 20 == 0:
                    print(f"â€¦å¹¶å‘è¿›åº¦ï¼š{done}/{len(tasks)} ä¸ªä»»åŠ¡å®Œæˆ")
        return results

# ----------- 7:2:1 æ‹†åˆ† -----------
def split_dataset(items: List[dict], ratios=(0.7, 0.2, 0.1)):
    random.shuffle(items)
    n = len(items)
    n_train = int(n * ratios[0])
    n_val = int(n * (ratios[0] + ratios[1]))
    return items[:n_train], items[n_train:n_val], items[n_val:]

# ------------------- ä¸»æµç¨‹ -------------------
def main():
    # 1) è¯»å–æ•°æ®
    data = load_all(FILES)
    if not data:
        print("âŒ æœªè¯»å–åˆ°ä»»ä½•æ ·æœ¬ã€‚")
        return

    # 2) æŒ‰â€œç¬¬ä¸€ä¸»æ„å›¾â€åˆ†æ¡¶
    buckets = defaultdict(list)
    unlabeled = []
    for it in data:
        labs = get_primary_labels(it, PRIMARY_FROM_NAMES)
        if labs:
            buckets[labs[0]].append(it)
        else:
            unlabeled.append(it)

    # æ‰“å°åŸå§‹åˆ†å¸ƒ
    print("\nğŸ“Š åŸå§‹ä¸»æ„å›¾åˆ†å¸ƒï¼š")
    for k, c in sorted(((k, len(v)) for k, v in buckets.items()), key=lambda x: -x[1]):
        print(f"  {k}: {c}")
    if unlabeled:
        print(f"  (æ— ä¸»æ„å›¾æ ·æœ¬): {len(unlabeled)} â€”â€” ä¸å‚ä¸")

    # 3) æ‰¾æœ€å¤§ id
    max_id = 0
    for it in data:
        if isinstance(it.get("id"), int):
            max_id = max(max_id, it["id"])
    next_id = max_id + 1

    # 4) è§„åˆ’ï¼šæ¯ç±»ç»Ÿä¸€åˆ° TARGET_PER_CLASS
    # å…ˆç¡®å®šæ¯ç±»éœ€è¦â€œä¸‹é‡‡æ ·/ä¿ç•™/ç”Ÿæˆâ€çš„ç­–ç•¥
    result_all = []
    gen_tasks: List[GenTask] = []
    total_need = 0

    for label, items in buckets.items():
        cur = len(items)
        target = TARGET_PER_CLASS

        if cur > target:
            keep = random.sample(items, target)
            result_all.extend(keep)
            print(f"ğŸ”½ ç±»åˆ«ã€Š{label}ã€‹ä¸‹é‡‡æ · {cur}â†’{target}ï¼Œä¸¢å¼ƒ {cur - target}")
        elif cur == target:
            result_all.extend(items)
            print(f"â– ç±»åˆ«ã€Š{label}ã€‹ä¿æŒ {cur}")
        else:
            need = target - cur
            print(f"ğŸ”§ ç±»åˆ«ã€Š{label}ã€‹éœ€è¡¥ {need} æ¡ï¼ˆQwen å¹¶è¡Œç”Ÿæˆï¼‰")
            total_need += need

            # å°†ç”Ÿæˆéœ€æ±‚åˆ†é…åˆ°æºæ ·æœ¬ä¸Šï¼Œæ¯æ¡æºæ ·æœ¬ä¸è¶…è¿‡ MAX_GEN_PER_SOURCE
            i, produced = 0, 0
            while produced < need and items:
                src = items[i % len(items)]
                i += 1
                k = min(MAX_GEN_PER_SOURCE, need - produced)
                gen_tasks.append(GenTask(src, label, k))
                produced += k

            # åŸå§‹æ ·æœ¬ä¹ŸåŠ å…¥
            result_all.extend(items)

    print(f"\nğŸ§® è®¡åˆ’ç”Ÿæˆæ€»é‡ï¼š{total_need} æ¡ï¼›å¹¶å‘åº¦ï¼š{CONCURRENCY}")

    # 5) å¹¶å‘ç”Ÿæˆ
    start_t = time.time()
    new_items = asyncio.run(run_parallel_generation(gen_tasks, next_id))
    used = len(new_items)
    print(f"âœ… å¹¶å‘ç”Ÿæˆå®Œæˆï¼šæ–°å¢ {used} æ¡ï¼Œç”¨æ—¶ {time.time() - start_t:.1f}s")

    # 6) åˆå¹¶å¹¶åšæœ€ç»ˆæ•°é‡å¯¹é½ï¼ˆä¸‡ä¸€å› å¤±è´¥/å»é‡å¯¼è‡´ä¸è¶³ï¼‰
    result_all.extend(new_items)

    # æ ¡éªŒæ¯ç±»æœ€ç»ˆæ•°é‡ï¼›è‹¥ä»ä¸è¶³ï¼Œåªèƒ½æç¤ºï¼ˆæˆ–å†æ¥ä¸€è½®ç”Ÿæˆâ€”â€”æ­¤å¤„åªæç¤ºï¼‰
    final_buckets = defaultdict(list)
    for it in result_all:
        labs = get_primary_labels(it, PRIMARY_FROM_NAMES)
        if labs:
            final_buckets[labs[0]].append(it)

    warn_short = []
    for label, items in final_buckets.items():
        if len(items) < TARGET_PER_CLASS:
            warn_short.append((label, len(items)))
    if warn_short:
        print("\nâš ï¸ ä¸‹åˆ—ç±»åˆ«æœªå®Œå…¨è¡¥è¶³ï¼ˆå¯èƒ½å› æœåŠ¡é™é€Ÿ/æ–‡æœ¬è¿‡çŸ­/é‡å¤è¿‡æ»¤ï¼‰ï¼š")
        for lb, cnt in warn_short:
            print(f"  {lb}: {cnt}/{TARGET_PER_CLASS}")

    # 7) 7:2:1 æ‹†åˆ†
    train, val, test = split_dataset(sum(final_buckets.values(), []), SPLIT_RATIOS)

    # 8) è¾“å‡ºæ–‡ä»¶
    with open(f"{OUTPUT_PREFIX}_train.json", "w", encoding="utf-8") as f:
        json.dump(train, f, ensure_ascii=False, indent=2)
    with open(f"{OUTPUT_PREFIX}_val.json", "w", encoding="utf-8") as f:
        json.dump(val, f, ensure_ascii=False, indent=2)
    with open(f"{OUTPUT_PREFIX}_test.json", "w", encoding="utf-8") as f:
        json.dump(test, f, ensure_ascii=False, indent=2)

    # 9) æ‰“å°æœ€ç»ˆåˆ†å¸ƒï¼ˆå…¨é‡ï¼‰
    def count_primary(items):
        c = Counter()
        for it in items:
            labs = get_primary_labels(it, PRIMARY_FROM_NAMES)
            if labs:
                c.update([labs[0]])
        return c

    total_c = count_primary(sum(final_buckets.values(), []))
    print("\nğŸ“Š ç»Ÿä¸€åï¼ˆå…¨é‡ï¼‰ä¸»æ„å›¾åˆ†å¸ƒï¼š")
    for k, v in sorted(total_c.items(), key=lambda x: -x[1]):
        print(f"  {k}: {v}")

    print("\nğŸ“¦ å·²ç”Ÿæˆï¼š")
    print(f"  {OUTPUT_PREFIX}_train.json  ï¼ˆ{len(train)} æ¡ï¼‰")
    print(f"  {OUTPUT_PREFIX}_dev.json    ï¼ˆ{len(val)} æ¡ï¼‰")
    print(f"  {OUTPUT_PREFIX}_test.json   ï¼ˆ{len(test)} æ¡ï¼‰")

if __name__ == "__main__":
    main()
